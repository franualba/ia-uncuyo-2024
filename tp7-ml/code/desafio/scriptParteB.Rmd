```{r}
### 1. Preparación del entorno
library(tidymodels)
library(tidyverse)
library(caret)
library(lightgbm)
library(xgboost)
library(stacks)
library(ggplot2)
library(mltools)
library(data.table)
library(dplyr)
library(pROC)
library(rsample)
library(workflows)
library(tune)


### 2. Lectura y limpieza de datos
# Leer los archivos CSV
train_full <- read_csv("arbolado-mza-dataset.csv")
test_data <- read_csv("arbolado-mza-dataset-test.csv")  # Nuevo conjunto de test

# Función de limpieza común para ambos datasets
clean_dataset <- function(data) {
  data %>%
    mutate_all(~ str_replace_all(., '"', '')) %>%
    mutate(
      altura = factor(altura, levels = c("Bajo (2 - 4 mts)", "Medio (4 - 8 mts)", "Alto (> 8 mts)")),
      diametro_tronco = factor(diametro_tronco, levels = c("Chico", "Mediano", "Grande")),
      # Convert inclinacion_peligrosa to logical if it exists in the dataset
      inclinacion_peligrosa = if ("inclinacion_peligrosa" %in% names(.)) {
        as.logical(as.numeric(inclinacion_peligrosa))
      } else {
        NULL
      }
    )
}

# Split original training data into train and validation
set.seed(42)
train_idx <- sample(1:nrow(train_full), 0.8 * nrow(train_full))
train_data <- train_full[train_idx, ]
valid_data <- train_full[-train_idx, ]

# Clean all datasets
train_data <- clean_dataset(train_data)
valid_data <- clean_dataset(valid_data)
test_data <- clean_dataset(test_data)

### 3. Feature Engineering
create_features <- function(data, is_training = TRUE, training_data = NULL) {
  # Basic features
  data <- data %>%
    mutate(
      long = as.numeric(as.character(long)),
      lat = as.numeric(as.character(lat)),
      area_seccion = as.numeric(as.character(area_seccion))
    )
  
  # Características de ubicación
  data <- data %>%
    group_by(seccion, nombre_seccion) %>%
    mutate(
      dist_centro_seccion = sqrt((long - mean(long))^2 + (lat - mean(lat))^2),
      densidad_arboles = n() / area_seccion
    ) %>%
    ungroup()
  
  # Handle ratio_arboles_peligrosos
  if ("inclinacion_peligrosa" %in% names(data)) {
    # If we have inclinacion_peligrosa in the current dataset, calculate directly
    data <- data %>%
      group_by(seccion, nombre_seccion) %>%
      mutate(
        ratio_arboles_peligrosos = mean(as.numeric(inclinacion_peligrosa))
      ) %>%
      ungroup()
  } else if (!is.null(training_data) && "inclinacion_peligrosa" %in% names(training_data)) {
    # If we don't have inclinacion_peligrosa but we have training data, use training data ratios
    training_ratios <- training_data %>%
      group_by(seccion, nombre_seccion) %>%
      summarise(
        ratio_arboles_peligrosos = mean(as.numeric(inclinacion_peligrosa)),
        .groups = 'drop'
      )
    
    # Join with current data
    data <- data %>%
      left_join(training_ratios, by = c("seccion", "nombre_seccion")) %>%
      mutate(
        # Fill NA values with mean ratio from training data
        ratio_arboles_peligrosos = replace_na(
          ratio_arboles_peligrosos, 
          mean(training_ratios$ratio_arboles_peligrosos)
        )
      )
  } else {
    # If we have neither, add a dummy column with mean value (0.5)
    warning("No source for ratio_arboles_peligrosos, using default value of 0.5")
    data$ratio_arboles_peligrosos <- 0.5
  }
  
  # Handle especies one-hot encoding
  if (is_training) {
    especies_levels <<- unique(data$especie)
    data$especie <- factor(data$especie, levels = especies_levels)
  } else {
    data <- data %>%
      mutate(especie = factor(ifelse(especie %in% especies_levels, especie, "OTHER"),
                             levels = c(especies_levels, "OTHER")))
  }
  
  # Create one-hot encoded columns
  especies_one_hot <- one_hot(as.data.table(data["especie"]))
  
  # Combine with original data
  data <- cbind(data, especies_one_hot) %>% select(-especie)
  
  return(data)
}

# Apply feature engineering
train_data <- create_features(train_data, is_training = TRUE)
valid_data <- create_features(valid_data, is_training = FALSE, training_data = train_data)
test_data <- create_features(test_data, is_training = FALSE, training_data = train_data)

### 4. Preparación de datos para modelado
prepare_data <- function(data, has_target = TRUE) {
  if (has_target) {
    x_data <- data %>% select(-inclinacion_peligrosa)
    y_data <- data$inclinacion_peligrosa
    return(list(x = x_data, y = y_data))
  } else {
    return(list(x = data))
  }
}

# Prepare all datasets
train_prepared <- prepare_data(train_data)
valid_prepared <- prepare_data(valid_data)
test_prepared <- prepare_data(test_data, has_target = FALSE)

# Define columns to scale explicitly
columns_to_scale <- c(
  "long", 
  "lat", 
  "area_seccion",
  "dist_centro_seccion",
  "densidad_arboles"
)

# Add ratio_arboles_peligrosos if it exists
if ("ratio_arboles_peligrosos" %in% names(train_prepared$x)) {
  columns_to_scale <- c(columns_to_scale, "ratio_arboles_peligrosos")
}

# Verify columns exist and are numeric
numeric_col_names <- columns_to_scale[columns_to_scale %in% names(train_prepared$x)]
numeric_col_names <- numeric_col_names[sapply(train_prepared$x[numeric_col_names], is.numeric)]

# Print columns being scaled for verification
cat("Scaling the following columns:\n")
print(numeric_col_names)

# Calculate scaling parameters from training data
train_scaling_params <- lapply(train_prepared$x[numeric_col_names], function(x) {
  list(mean = mean(x, na.rm = TRUE),
       sd = sd(x, na.rm = TRUE))
})

# Function to scale data using training parameters
scale_features <- function(data, scaling_params, numeric_cols) {
  scaled_data <- data
  for (col in numeric_cols) {
    if (col %in% names(data) && col %in% names(scaling_params)) {
      params <- scaling_params[[col]]
      scaled_data[[col]] <- (data[[col]] - params$mean) / params$sd
    }
  }
  return(scaled_data)
}

# Scale all datasets
train_prepared$x <- scale_features(train_prepared$x, train_scaling_params, numeric_col_names)
valid_prepared$x <- scale_features(valid_prepared$x, train_scaling_params, numeric_col_names)
test_prepared$x <- scale_features(test_prepared$x, train_scaling_params, numeric_col_names)

### 5. Modelado
prepare_matrix_data <- function(df) {
  df <- df %>% select_if(function(x) is.numeric(x) | is.logical(x))
  df <- df %>% mutate_if(is.logical, as.numeric)
  return(as.matrix(df))
}

# Prepare final training data
x_train_matrix <- prepare_matrix_data(train_prepared$x)
train_combined <- bind_cols(
  as.data.frame(x_train_matrix),
  inclinacion_peligrosa = factor(train_prepared$y)
)

# Set up cross-validation
set.seed(42)
cv_folds <- vfold_cv(train_combined, v = 5)
```


```{r}
# Define model
lgbm_spec <- boost_tree(
  trees = 100,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune()
) %>%
  set_engine("lightgbm") %>%
  set_mode("classification")

# Create workflow
lgbm_wf <- workflow() %>%
  add_model(lgbm_spec) %>%
  add_formula(inclinacion_peligrosa ~ .)

# Define parameter grid
tree_grid <- grid_space_filling(
  min_n(),
  tree_depth(),
  learn_rate(),
  size = 5
)

# Add control settings
ctrl_stack <- control_stack_grid()
ctrl_res <- control_grid(
  save_pred = TRUE,
  save_workflow = TRUE
)

# Tune model
lgbm_res <- tune_grid(
  lgbm_wf,
  resamples = cv_folds,
  grid = tree_grid,
  metrics = metric_set(roc_auc),
  control = ctrl_res
)

# Create and train stacked model
stack_model <- stacks(control = ctrl_stack) %>%
  add_candidates(lgbm_res) %>%
  blend_predictions() %>%
  fit_members()
```


```{r}
### 6. Evaluación del modelo en el conjunto de validación

# Prepare validation data for prediction
x_valid_matrix <- prepare_matrix_data(valid_prepared$x)
valid_combined <- as.data.frame(x_valid_matrix)

# Make predictions on validation set
valid_predictions_raw <- predict(stack_model, valid_combined, type = "prob")

# Extract probabilities for positive class
# First, check the structure of predictions
print("Structure of predictions:")
str(valid_predictions_raw)

# Convert target variable to numeric (0/1)
valid_target <- as.numeric(valid_prepared$y)

# Extract probabilities (adjusting based on column names)
if ("1" %in% names(valid_predictions_raw)) {
  valid_predictions <- valid_predictions_raw[["1"]]
} else if ("TRUE" %in% names(valid_predictions_raw)) {
  valid_predictions <- valid_predictions_raw[["TRUE"]]
} else if (".pred_1" %in% names(valid_predictions_raw)) {
  valid_predictions <- valid_predictions_raw[[".pred_1"]]
} else if (".pred_TRUE" %in% names(valid_predictions_raw)) {
  valid_predictions <- valid_predictions_raw[[".pred_TRUE"]]
} else {
  # If none of the expected column names are found, print available columns
  print("Available prediction columns:")
  print(names(valid_predictions_raw))
  stop("Could not find probability column in predictions")
}

# Print summary of predictions and target
print("Summary of predictions:")
summary(valid_predictions)
print("Summary of target variable:")
summary(valid_target)

# Check for NAs
if (any(is.na(valid_predictions))) {
  warning("NAs found in predictions")
  valid_predictions[is.na(valid_predictions)] <- 0.5  # or another appropriate value
}
if (any(is.na(valid_target))) {
  warning("NAs found in target variable")
  valid_target <- valid_target[!is.na(valid_target)]
  valid_predictions <- valid_predictions[!is.na(valid_target)]
}

# Calculate and print AUC-ROC
tryCatch({
  auc_roc <- roc(valid_target, valid_predictions)$auc
  cat(sprintf("\nAUC-ROC en el conjunto de validación: %.3f\n", auc_roc))
  
  # Create ROC curve plot
  roc_obj <- roc(valid_target, valid_predictions)
  plot(roc_obj, main = "Curva ROC")
}, error = function(e) {
  cat("Error calculating ROC curve:", conditionMessage(e), "\n")
  print("Unique values in target:")
  print(table(valid_target))
  print("Range of predictions:")
  print(range(valid_predictions))
})

### 7. Predicciones sobre el conjunto de test
# Prepare test data for prediction
x_test_matrix <- prepare_matrix_data(test_prepared$x)
test_combined <- as.data.frame(x_test_matrix)

# Make predictions
test_predictions_raw <- predict(stack_model, test_combined, type = "prob")

# Extract probabilities for test set (using same logic as validation)
if ("1" %in% names(test_predictions_raw)) {
  test_predictions <- test_predictions_raw[["1"]]
} else if ("TRUE" %in% names(test_predictions_raw)) {
  test_predictions <- test_predictions_raw[["TRUE"]]
} else if (".pred_1" %in% names(test_predictions_raw)) {
  test_predictions <- test_predictions_raw[[".pred_1"]]
} else if (".pred_TRUE" %in% names(test_predictions_raw)) {
  test_predictions <- test_predictions_raw[[".pred_TRUE"]]
} else {
  print("Available prediction columns for test:")
  print(names(test_predictions_raw))
  stop("Could not find probability column in test predictions")
}

# Create results dataframe
results_df <- data.frame(
  id = test_data$id,
  inclinacion_peligrosa = test_predictions
)

# Save predictions
write.csv(results_df, "predictions.csv", row.names = FALSE, quote = FALSE)

```

```{r}
### Threshold Optimization

# Calculate ROC curve
roc_obj <- roc(valid_target, valid_predictions)

# Find optimal threshold using Youden's J statistic
# This maximizes the balance between sensitivity and specificity
coords <- coords(roc_obj, "best", best.method="youden")
optimal_threshold <- coords$threshold

# Print threshold and performance metrics
cat(sprintf("\nOptimal threshold: %.3f\n", optimal_threshold))
cat(sprintf("At this threshold:\n"))
cat(sprintf("Sensitivity: %.3f\n", coords$sensitivity))
cat(sprintf("Specificity: %.3f\n", coords$specificity))

# Create confusion matrix for validation set
valid_predicted_classes <- factor(ifelse(valid_predictions >= optimal_threshold, 1, 0))
valid_actual_classes <- factor(valid_target)
conf_matrix <- confusionMatrix(valid_predicted_classes, valid_actual_classes)

# Print confusion matrix and additional metrics
print(conf_matrix)

# Visualize ROC curve with optimal threshold
plot(roc_obj, main = "ROC Curve with Optimal Threshold")
points(coords$specificity, coords$sensitivity, pch = 19, col = "red")
text(coords$specificity, coords$sensitivity, 
     labels = sprintf("\nThreshold: %.2f", optimal_threshold),
     pos = 4, col = "red")

### Apply to test predictions and save binary results
# Convert predictions to binary using optimal threshold
binary_test_predictions <- as.numeric(test_predictions >= optimal_threshold)

# Create results dataframe with binary predictions
results_df <- data.frame(
  id = test_data$id,
  inclinacion_peligrosa = binary_test_predictions
)

# Save binary predictions
write.csv(results_df, "predictions_binary.csv", row.names = FALSE, quote = FALSE)

# Print distribution of predictions
cat("\nDistribution of binary predictions in test set:\n")
print(table(binary_test_predictions))
print(prop.table(table(binary_test_predictions)))
```

